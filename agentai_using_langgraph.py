# -*- coding: utf-8 -*-
"""AgentAI_using_LangGraph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11YliZ8ccFmfTccSiv7L0BxjrzFZCsnnD
"""

!pip install langchain langgraph langchain_huggingface transformers accelerate sentencepiece

!git config --global credential.helper store

from huggingface_hub import login

login(add_to_git_credential=True)

#load llama2

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_huggingface import HuggingFacePipeline
import torch

model_name = "meta-llama/Llama-2-7b-chat-hf"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
)

llm = HuggingFacePipeline(pipeline=pipe)

# Agent State

class AgentState(TypedDict):
    messages: list
    tool_result: Optional[str]

# Nodes

def llm_node(state: AgentState):
    """Send messages to Llama2 chat model"""
    prompt = ""

    for msg in state["messages"]:
        if msg["role"] == "user":
            prompt += f"User: {msg['content']}\n"
        elif msg["role"] == "assistant":
            prompt += f"Assistant: {msg['content']}\n"
        elif msg["role"] == "tool":
            prompt += f"Tool: {msg['content']}\n"

    prompt += "Assistant:"

    response = pipe(prompt)[0]["generated_text"]

    # Extract ONLY the assistant answer
    answer = response[len(prompt):].strip()

    return {
        "messages": state["messages"] + [{"role": "assistant", "content": answer}],
        "tool_result": None
    }


def tool_node(state: AgentState):
    """Fake tool for example"""
    last_message = state["messages"][-1]["content"]

    tool_result = f"[TOOL EXECUTED] You asked: {last_message}"

    return {
        "messages": state["messages"] + [{"role": "tool", "content": tool_result}],
        "tool_result": tool_result
    }


def final_node(state: AgentState):
    """Return final assistant answer"""
    return {
        "messages": state["messages"] + [
            {"role": "assistant", "content": f"Final Answer: {state['tool_result']}"}
        ],
        "tool_result": state["tool_result"]
    }

#define tools for agent

def tool_calculator(q: str):
  try:
    return str(eval(q))
  except:
    return "math error"

def tool_search(q: str):
  return f"(fake search result) You asked: {q}"


tools = {
    "calculator": tool_calculator,
    "search": tool_search,
}

#agent state define

from typing import TypedDict, Optional

class AgentState(TypedDict):
    question: str
    thought: Optional[str]
    action: Optional[str]
    action_input: Optional[str]
    observation: Optional[str]
    final_answer: Optional[str]

#build langGraph

from langgraph.graph import StateGraph, END

graph = StateGraph(AgentState)

graph.add_node("llm", llm_node)
graph.add_node("tool", tool_node)
graph.add_node("final", final_node)

graph.set_entry_point("llm")

graph.add_edge("llm", "tool")
graph.add_edge("tool", "final")
graph.add_edge("final", END)

agent = graph.compile()

state = {
    "messages": [{"role": "user", "content": "What is LangGraph?"}],
    "tool_result": None
}

result = agent.invoke(state)
print(result["messages"][-1]["content"])

